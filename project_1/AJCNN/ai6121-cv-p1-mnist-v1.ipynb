{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ai6121-cv-p1-mnist-v1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rYuyP6vEey8u"},"source":["# AI6121 Computer Vision Project 1: MNIST"]},{"cell_type":"code","metadata":{"id":"ULNHg0qae5wX"},"source":["ver = 1\n","comments = ''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qNQjAQ5jB_Z"},"source":["## Versioning, Changelogs & References"]},{"cell_type":"markdown","metadata":{"id":"dRSJq47gjIEm"},"source":["### Changelogs\n","+ V0.1 - Base codes with:\n","    + Config setting, seeding, checkpoint saving, criterion, optimizer, scheduler setting, dataset loading and model training\n","\n","+ V0.2 - Fixed bug in ACC computation in utils/train_helper.py\n","  + Added LeNet5 in models/BasicCNN.py\n","  + Added code to train full training set with best hyper-parameters from cross validation \n","\n","+ V0.3 - Checkpoint Update\n","  + Added data_augmentation() function for easy augmentation tuning.\n","  + Moved checkpoints saving to be based on run_time for parallel training.\n","\n","+ V0.4 - Visualization Update\n","  + Added AJCNN to create model options\n","  + Added TorchViz\n","  + Added Tensorboard extensions to Notebook\n","\n","+ V0.5 - Augmentations Update\n","  + Added Torchvision Data Augmentations\n","  + Added Albumentations Data Augmentations:\n","    + ScaleShiftRotate\n","    + Distortion\n","    + ElasticTransform (Elastic Distortion) [Simard2003]\n","\n","+ V0.6 - Error Analysis Update\n","  + Added Error Analysis for Testset.\n","    + Tabulate accuracy scores.\n","    + Plot mis-classified examples.\n","\n","+ V1.0 - Cleanup for Submission\n"]},{"cell_type":"markdown","metadata":{"id":"A5RDPsgmjTrc"},"source":["### References\n","+ [Official MNIST](https://yann.lecun.com/exdb/mnist/)\n","+ [PyTorch MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)"]},{"cell_type":"markdown","metadata":{"id":"FQGNGAEefFDL"},"source":["## Setup/ Configuration"]},{"cell_type":"markdown","metadata":{"id":"qrv95mmcctvi"},"source":["### 3rd Party Installations"]},{"cell_type":"code","metadata":{"id":"-a8CwhbFcyBf"},"source":["!pip install torchsummary\n","!pip install torchviz\n","!pip install albumentations==0.5.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dnx5Wd07jrj3"},"source":["### Google Colab Setup"]},{"cell_type":"code","metadata":{"id":"L6X75vecCBEd"},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","    file_name = f'ai6121-cv-p1-mnist-v{ver}.ipynb'\n","    import subprocess\n","    path_to_file = subprocess.check_output('find . -type f -name ' + str(file_name), shell=True).decode(\"utf-8\")\n","    #path_to_file = f'/content/gdrive/My Drive/AI6121 - CV Project/ai6121-cv-p1-mnist-v{ver}.ipynb'\n","    path_to_file = path_to_file.replace(file_name,\"\").replace('\\n',\"\")\n","    print(path_to_file)\n","    os.chdir(path_to_file)\n","    !pwd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPntdWgrj1CO"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"hdPSzZ2ffxDf"},"source":["import random\n","import time\n","import numpy as np\n","from pprint import pprint\n","from tqdm import tqdm\n","import shutil\n","from datetime import datetime\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import lr_scheduler\n","import torch.optim as optim\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","from PIL import Image\n","\n","from torchsummary import summary\n","from torchviz import make_dot\n","\n","import utils\n","from utils import ModelTimer, AverageMeter\n","import models\n","from dataset.MNIST import MNISTDataset\n","from IPython.core.debugger import set_trace\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","sns.set()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qj2kIzGRjG-o"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# set the backend of matplotlib to the 'inline' backend\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O43JtwSqkjCO"},"source":["# Test imports\n","print(torch.__version__, \"Cuda Avail: \", torch.cuda.is_available())\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'torch device: {device}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GThDCr7hmsnT"},"source":["import importlib\n","importlib.reload(utils)\n","importlib.reload(models)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZ9ZO9sBj5Us"},"source":["### Configurations"]},{"cell_type":"code","metadata":{"id":"WrB-Te_cmkwu"},"source":["config = utils.load_config_from_file('config.ini')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qU7XsdaueyFB"},"source":["# Manually update configurations (string) \n","config.set('CONSTANTS','manual_seed', '42')\n","config.set('CONSTANTS','evaluate', 'False')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZxGmDhWnujD"},"source":["if False: # update/save to config file (True / False)\n","    utils.update_config_to_file('config.ini', config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kTGgl4i3TvL"},"source":["pConfig = config['PATHS']\n","cConfig = config['CONSTANTS']\n","dConfig = config['DEFAULT']\n","trainCfg = config['TRAIN']\n","valCfg = config['VAL']\n","testCfg = config['TEST']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJ0YPCbKk4Tz"},"source":["### Seeding"]},{"cell_type":"code","metadata":{"id":"q6Pwy4Chk5dC"},"source":["# set random seed for reproducibility\n","def seed_everything(seed=None):\n","    if seed is None:\n","        seed = random.randint(1, 10000) # create random seed\n","        print(f'random seed used: {seed}')\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    if 'torch' in sys.modules:\n","        print(f\"seeding torch modules\")\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","    return seed\n","    \n","seed = seed_everything(seed=cConfig.getint('manual_seed'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RS3MDy8ej89S"},"source":["## Dataset, DataLoaders"]},{"cell_type":"code","metadata":{"id":"j_Ez352Cczsm"},"source":["def data_augmentations(augments=None, albumentation=False):\n","    if not albumentation:\n","        normalize = transforms.Normalize((0.1307,), (0.3081,))\n","        data_transforms = {\n","            'train': transforms.Compose([\n","                transforms.ToTensor(),\n","                normalize\n","            ]),\n","            'valid': transforms.Compose([\n","                transforms.ToTensor(),\n","                normalize\n","            ]),\n","            'test': transforms.Compose([\n","                transforms.ToTensor(),\n","                normalize\n","            ])\n","        }\n","        if augments is not None:\n","            all_augs = []\n","                \n","            all_augs += [transforms.ToTensor(), normalize]\n","\n","            if 'RandomRotation' in augments: \n","                all_augs.append(transforms.RandomRotation(degrees=10,\n","                                                          resample=Image.BILINEAR))\n","            if 'RandomAffine' in augments: \n","                all_augs.append(transforms.RandomAffine(degrees=10, \n","                                                        resample=Image.BILINEAR))\n","            \n","            if 'RandomPerspective' in augments: \n","                all_augs.append(transforms.RandomPerspective(distortion_scale=0.2, p=0.5, \n","                                                             interpolation=Image.BILINEAR))\n","\n","            if 'RandomResizedCrop' in augments: \n","                all_augs.append(transforms.RandomResizedCrop(size=(28,28)))\n","                \n","            if 'RandomErasing' in augments: # only random erasing is after to tensor\n","                all_augs.append(transforms.RandomErasing(p=0.3, scale=(0.02, 0.33), \n","                                                         ratio=(0.3, 3.3), value=0))\n","            if 'RandomCropAndResize' in augments:   \n","                all_augs.append(transforms.RandomApply(torch.nn.ModuleList(\n","                    [ transforms.RandomCrop(24), transforms.Resize(28)]), p=0.5))   \n","\n","            data_transforms['train'] = transforms.Compose(all_augs)\n","    else:\n","        normalizeA = A.Normalize(mean=[0.1307,],std=[0.3081,])\n","        data_transforms = {\n","            'train': A.Compose([\n","                normalizeA,\n","                ToTensorV2()\n","            ]),\n","            'valid': A.Compose([\n","                normalizeA,\n","                ToTensorV2()\n","            ]),\n","            'test': A.Compose([\n","                normalizeA,\n","                ToTensorV2()\n","            ])\n","        }\n","        if augments is not None:\n","            all_augs = []\n","            if 'ShiftScaleRotate' in augments:\n","                all_augs.append(A.ShiftScaleRotate(shift_limit = 0.1,\n","                                                    scale_limit = 0.1,\n","                                                    rotate_limit = 20,\n","                                                    interpolation = cv2.INTER_LANCZOS4,\n","                                                    border_mode = cv2.BORDER_CONSTANT,\n","                                                    p = 1))\n","            if 'Distortion' in augments:\n","                all_augs.append(A.OneOf([A.OpticalDistortion(border_mode = cv2.BORDER_CONSTANT, p=1.0),\n","                                         A.GridDistortion(border_mode = cv2.BORDER_CONSTANT,p=1.0)],\n","                                        p=0.5))\n","            if 'ElasticTransform' in augments: \n","                all_augs.append(A.ElasticTransform(alpha=8, sigma=3, alpha_affine=2, p=0.5))\n","                \n","            all_augs += [normalizeA, ToTensorV2()]\n","            data_transforms['train'] = A.Compose(all_augs)\n","\n","    return data_transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xk0SIQHJgFsy"},"source":["# trainCfg['augments'] = 'RandomRotation' #'RandomRotation,RandomCropAndResize'\n","augCfg = None\n","if not trainCfg['augments'] == 'None':\n","    augCfg = trainCfg['augments'].split(',')\n","print(augCfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfK7j2eHdz31"},"source":["test_da = data_augmentations(augCfg)\n","print(test_da)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6e93vQfeiZgo"},"source":["def load_mnist_datasets(augments = None, verbose=True, visualize=None):\n","    data_transforms = data_augmentations(augments)\n","    print(data_transforms)\n","\n","    mnist_datasets = {}\n","    dataloaders = {}\n","    if dConfig.getboolean('evaluate'): # Load test set only\n","        test_dataset = MNISTDataset(root=pConfig['datapath'], train=False, \n","                                    download=True, transform=data_transforms['test'])\n","\n","        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=testCfg.getint('batch'), \n","                       num_workers=dConfig.getint('num_workers'), pin_memory=dConfig.getboolean('pin_memory'))\n","        \n","        mnist_datasets['test'] = test_dataset\n","        dataloaders['test'] = test_loader\n","        \n","    else:\n","        train_dataset = MNISTDataset(pConfig['datapath'], train=True, \n","                                     download=True, transform=data_transforms['train'])\n","        if valCfg.getfloat('split') > 0:\n","            # split validation set from train set\n","            valid_dataset = MNISTDataset(root=pConfig['datapath'], train=True, \n","                                          download=True, transform=data_transforms['valid'])\n","            \n","            num_train = len(train_dataset)\n","            indices = list(range(num_train))\n","            np.random.shuffle(indices) # shuffle indices\n","            split = int(np.floor(valCfg.getfloat('split') * num_train))\n","            \n","            train_indices, valid_indices = indices[split:], indices[:split]\n","            train_sampler = SubsetRandomSampler(train_indices)\n","            valid_sampler = SubsetRandomSampler(valid_indices)\n","            \n","            mnist_datasets['train'] = torch.utils.data.Subset(train_dataset, train_indices)\n","            mnist_datasets['valid'] = torch.utils.data.Subset(valid_dataset, valid_indices)\n","        else:\n","            # Use test set as validation set\n","            print(f\"Using test set as validation set!\")\n","            valid_dataset = MNISTDataset(root=pConfig['datapath'], train=False, \n","                                   download=True, transform=data_transforms['valid'])\n","            \n","            train_sampler = None\n","            valid_sampler = None\n","            mnist_datasets['train'] = train_dataset\n","            mnist_datasets['valid'] = valid_dataset\n","\n","        train_loader = torch.utils.data.DataLoader(train_dataset, \n","                      batch_size=trainCfg.getint('batch'), sampler=train_sampler, \n","                      num_workers=dConfig.getint('num_workers'), \n","                      pin_memory=dConfig.getboolean('pin_memory'))\n","\n","        valid_loader = torch.utils.data.DataLoader(valid_dataset, \n","                      batch_size=valCfg.getint('batch'), sampler=valid_sampler, \n","                      num_workers=dConfig.getint('num_workers'), \n","                      pin_memory=dConfig.getboolean('pin_memory'))\n","\n","        dataloaders['train'] = train_loader\n","        dataloaders['valid'] = valid_loader\n","        \n","    if verbose:\n","        dataset_sizes = {x: len(mnist_datasets[x]) for x in mnist_datasets.keys()}\n","        print(f\"Dataset sizes: {dataset_sizes}\")\n","\n","    if visualize is not None:\n","        sample_loader = torch.utils.data.DataLoader(mnist_datasets[visualize], \n","                                                    batch_size=9, shuffle=True, \n","                                                    num_workers=dConfig.getint('num_workers'), \n","                                                    pin_memory=dConfig.getboolean('pin_memory'))\n","        for idx, (images, labels) in enumerate(sample_loader):\n","            X = images.numpy()\n","            utils.plot_images(X, labels)\n","            break\n","\n","    return mnist_datasets, dataloaders\n","\n","dConfig['evaluate'] = 'False'\n","mnist_datasets, dataloaders = load_mnist_datasets(augments = augCfg, visualize='train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0mIdCU7JeDj"},"source":["## Create Model"]},{"cell_type":"code","metadata":{"id":"cfc9mIu1eyHW"},"source":["model_names = sorted(name for name in models.__dict__\n","                     if callable(models.__dict__[name]))\n","print(f\"Available Models: {model_names}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3yFSY7JeyJf"},"source":["def create_model(arch, device, verbose=True, plot=False):\n","    print(\"=> creating model '{}'\".format(arch))\n","    if arch.startswith('LeNet5'):\n","\n","        # Original LeNet5 arch default params\n","        kwargs = {}\n","        \n","        # Original LeNet5 arch params with relu and max pool\n","        # kwargs = {'activation':'relu', 'pool':'max'}\n","\n","        # LeNet5 arch with more filters\n","        # kwargs = {'kernel':3, 'pad':1, \n","        #          'activation':'relu', 'pool':'max',\n","        #          'num_filter1':50, 'num_filter2':100,\n","        #          'linear1':3600}\n","\n","        model = models.__dict__[arch](**kwargs)\n","    elif arch.startswith('VGG'):\n","        model = models.__dict__['VGG'](variant=arch, batch_norm=False)\n","    elif arch.startswith('AJCNN'):\n","        model = models.__dict__['AJCNN'](variant=arch)\n","    else:\n","        model = models.__dict__[arch]()\n","    model = model.to(device)\n","    \n","    if verbose:\n","        print(model)\n","        summary(model, (1,28,28))\n","    if plot:\n","        eg_input = torch.zeros((64, 1, 28, 28), \n","                               dtype=torch.float, requires_grad=False).to(device)\n","        y = model(eg_input)\n","        make_dot(y, params=dict(list(model.named_parameters()))).render(arch, format=\"png\")\n","    return model\n","\n","model = create_model(dConfig['model'], device, verbose=True, plot=False)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ml4LfSGHuzz2"},"source":["### Save Checkpoint"]},{"cell_type":"markdown","metadata":{"id":"1D6ZZRiMvwuG"},"source":[""]},{"cell_type":"code","metadata":{"id":"0ptuAQJuuzQF"},"source":["def save_ckp(state, is_best, checkpoint_path, bestmodel_path):\n","    print(f\"=> saving checkpoint '{checkpoint_path}'\")\n","    torch.save(state, checkpoint_path)\n","    if is_best:\n","        print(f\"=> saving best model '{bestmodel_path}'\")\n","        # copy that checkpoint file to best path given, bestmodel_path\n","        shutil.copyfile(checkpoint_path, bestmodel_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sp_O1lZJQRmn"},"source":["### Load Checkpoint"]},{"cell_type":"code","metadata":{"id":"SWUjh4AdQTyX"},"source":["def load_ckp(checkpoint_path, device, optimizer=None):\n","    print(f\"=> loading checkpoint '{checkpoint_path}'\")\n","    checkpoint = torch.load(checkpoint_path)\n","    print(f\"=> creating model '{checkpoint['arch']}'\")\n","    model = create_model(checkpoint['arch'], device)\n","    model.load_state_dict(checkpoint['state_dict'])\n","\n","    epoch = checkpoint['epoch']\n","    best_val_acc = checkpoint['best_val_acc']\n","\n","    if optimizer:\n","        optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","    lr = checkpoint['lr']\n","    total_time = checkpoint['total_time']\n","    model_timer = ModelTimer(total_time)\n","    print(f\"=> loaded checkpoint '{checkpoint_path}' (epoch {epoch})\")\n","    print(f\"=> checkpoint's best val '{best_val_acc}' ({model_timer})\")\n","    return model, best_val_acc, total_time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"08WuV7TEXVRc"},"source":["## Criterion, Optimizer, Scheduler"]},{"cell_type":"code","metadata":{"id":"abujBHKT3V0o"},"source":["class SmoothCrossEntropyLoss(nn.Module):\n","    def __init__(self, smoothing=0.0):\n","        super(SmoothCrossEntropyLoss, self).__init__()\n","        self.smoothing = smoothing\n","    \n","    def forward(self, input, target):\n","        log_prob = F.log_softmax(input, dim=-1)\n","        weight = input.new_ones(input.size()) * \\\n","            self.smoothing / (input.size(-1) - 1.)\n","        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n","        loss = (-weight * log_prob).sum(dim=-1).mean()\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDlrK6ZAeyL1"},"source":["def get_criterion(loss_name, device):\n","    criterion = nn.CrossEntropyLoss().to(device)\n","    if loss_name.startswith('SmoothCrossEntropyLoss'):\n","        criterion = SmoothCrossEntropyLoss(smoothing=0.003).to(device)\n","    return criterion\n","\n","# criterion = get_criterion(config['DEFAULT']['criterion'], device)\n","# criterion"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjOV4T2GeyOH"},"source":["def get_optimizer(model, opt_name, config=config):\n","    print(\"=> initializing optimizer '{}'\".format(opt_name))\n","    optimizer = None\n","    parameters = model.parameters()\n","    lr = config.getfloat('DEFAULT', 'lr')\n","    mom = config.getfloat('DEFAULT', 'momentum')\n","    wd = config.getfloat('DEFAULT', 'weight_decay')\n","    if opt_name == 'SGD':\n","        optimizer = optim.SGD(parameters, lr, momentum=mom, weight_decay=wd)\n","    elif opt_name == 'Adam':\n","        optimizer = optim.Adam(parameters, lr, weight_decay=wd)\n","    elif opt_name == 'AdamW':\n","        optimizer = optim.AdamW(parameters, lr, weight_decay=wd)\n","\n","    return optimizer\n","\n","# config['DEFAULT']['optimizer'] = 'SGD'\n","# config['DEFAULT']['lr'] = '0.001'\n","optimizer = get_optimizer(model, config['DEFAULT']['optimizer'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzxZg2MXeyQd"},"source":["def get_scheduler(optimizer, sch_name, config=trainCfg):\n","    print(\"=> initializing scheduler '{}'\".format(sch_name))\n","    \n","    scheduler = None # Manual\n","    if sch_name == 'StepLR':\n","        step = config.getint('step_size')\n","        sgamma = config.getfloat('step_gamma')\n","        scheduler = lr_scheduler.StepLR(optimizer, step_size=step, gamma=sgamma)\n","    elif sch_name == 'MultiStepLR':\n","        sgamma = config.getfloat('step_gamma')\n","        scheduler = lr_scheduler.MultiStepLR(optimizer,  milestones=[30,60], gamma=sgamma)\n","    elif sch_name == 'ReduceLROnPlateau':\n","        patience = config.getfloat('plateau_patience')\n","        plat_factor = config.getfloat('plateau_factor')\n","        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n","                                                  factor=plat_factor,\n","                                                  patience=patience)\n","    return scheduler    \n","\n","scheduler = get_scheduler(optimizer, trainCfg['scheduler'], config=trainCfg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-yV6RqNkb51g"},"source":["## Train/ Validation Functions\n"]},{"cell_type":"code","metadata":{"id":"aO_MZyL1eySs"},"source":["def train(train_loader, model, criterion, optimizer, device):\n","    model.train()\n","    \n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    corrects = AverageMeter()\n","\n","    end = time.time()\n","    for i, (X, y) in enumerate(tqdm(train_loader)):\n","          X = X.to(device, non_blocking=True)\n","          y = y.to(device, non_blocking=True)\n","\n","          X.requires_grad_()\n","          \n","          optimizer.zero_grad()\n","          outputs = model(X)\n","          loss = criterion(outputs, y)\n","\n","          losses.update(loss.detach().item(), X.size(0))\n","          corrects.update(utils.get_error(outputs.detach(), y))\n","\n","          # compute bp\n","          loss.backward()\n","          optimizer.step()\n","\n","          # measure elapsed time\n","          batch_time.update(time.time() - end)\n","          end = time.time()\n","          \n","          if (i + 1)% 100 == 0:\n","              print_line = '[Train] ({batch}/{size}) Batch: {bt:.3f}s | Loss: {loss:.6f} | Acc: {acc: .3f}'.format(\n","                          batch=i + 1,\n","                          size=len(train_loader),\n","                          bt=batch_time.avg,\n","                          loss=losses.avg,\n","                          acc=corrects.avg*100)\n","              print(print_line)\n","\n","    return losses, corrects"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6R-TylNseyVD"},"source":["def validate(valid_loader, model, criterion, device):\n","    model.eval()\n","    \n","    batch_time = AverageMeter()\n","    losses = AverageMeter()\n","    corrects = AverageMeter()\n","    i = 0\n","    with torch.no_grad():\n","        end = time.time()\n","        for i, (X, y) in enumerate(tqdm(valid_loader)):\n","              X = X.to(device, non_blocking=True)\n","              y = y.to(device, non_blocking=True)\n","              \n","              outputs = model(X)\n","              loss = criterion(outputs, y)\n","\n","              losses.update(loss.detach().item(), X.size(0)) \n","              corrects.update(utils.get_error(outputs.detach(), y))\n","\n","              # measure elapsed time\n","              batch_time.update(time.time() - end)\n","              end = time.time()\n","\n","    print_line = '[Test] ({batch}/{size}) Batch: {bt:.3f}s | Loss: {loss:.6f} | Acc: {acc: .3f}'.format(\n","                batch=i + 1,\n","                size=len(valid_loader),\n","                bt=batch_time.avg,\n","                loss=losses.avg,\n","                acc=corrects.avg*100)\n","    print(print_line)\n","    return losses, corrects"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"maoEkIS0pVS3"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"ku-rdc-6eyXe"},"source":["def trainer(dataloaders, model, criterion, optimizer, device, run_name, config=config):\n","    # visualization\n","    writer = SummaryWriter(os.path.join(pConfig['tensorboard_dir'], run_name))\n","    model_timer = ModelTimer()\n","    scheduler = get_scheduler(optimizer, config['TRAIN']['scheduler'], config=config['TRAIN'])\n","    \n","    stagnant_val_loss_ctr = 0\n","    min_val_loss = 1.\n","    epochs = trainCfg.getint('epochs')\n","    best_val_acc = 0\n","    for epoch in range(epochs):\n","        model_timer.start_epoch_timer()\n","        \n","        lr = optimizer.param_groups[0]['lr']\n","\n","        print('\\nEpoch: [%d | %d] LR: %.16f' % (epoch + 1, epochs, lr))\n","\n","        # train for one epoch\n","        train_losses, train_accs = train(dataloaders['train'], model, criterion, \n","                                         optimizer, device)\n","        \n","        # evaluate on validation set\n","        val_losses, val_accs = validate(dataloaders['valid'], model, criterion, \n","                                        device)\n","        \n","        if config['TRAIN']['scheduler'] == 'ReduceLROnPlateau':\n","            scheduler.step(val_losses.avg)\n","        else:\n","            scheduler.step()  \n","\n","        # tensorboardX\n","        writer.add_scalar('learning rate', lr, epoch + 1)\n","        writer.add_scalars('loss', {'train loss': train_losses.avg, \n","                                    'validation loss': val_losses.avg}, epoch + 1)\n","        writer.add_scalars('accuracy', {'train accuracy': train_accs.avg*100, \n","                                        'validation accuracy': val_accs.avg*100}, epoch + 1)\n","\n","        is_best = val_accs.avg > best_val_acc\n","        best_val_acc = max(val_accs.avg, best_val_acc)\n","        model_timer.stop_epoch_timer()\n","        save_ckp({\n","            'epoch': epoch + 1,\n","            'arch': model.name,\n","            'state_dict': model.state_dict(),\n","            'best_val_acc': best_val_acc*100,\n","            'opt_name': config[\"DEFAULT\"][\"optimizer\"],\n","            'optimizer' : optimizer.state_dict(),\n","            'lr': lr,\n","            'total_time': model_timer.total_time,\n","            'scheduler': config[\"TRAIN\"][\"scheduler\"],\n","            'criterion': config[\"DEFAULT\"][\"criterion\"]\n","        }, is_best, pConfig['checkpoint_fname'], pConfig['bestmodel_fname'])\n","        \n","        if trainCfg.getboolean('early_stopping'):\n","            if is_best:\n","                stagnant_val_loss_ctr = 0\n","                min_val_loss = val_losses.avg\n","            elif val_losses.avg >= min_val_loss:\n","                stagnant_val_loss_ctr += 1\n","                if (epoch+1) > trainCfg.getint('es_min') and stagnant_val_loss_ctr >= trainCfg.getint('es_patience'): \n","                    break\n","            else:\n","                stagnant_val_loss_ctr = 0\n","                min_val_loss = val_losses.avg\n","\n","    print(\"Training completed!\")\n","    writer.close()\n","    \n","    print(f'Best accuracy: {best_val_acc*100}')\n","    return model_timer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zzJYWzEX5Ylq"},"source":["## Main Function"]},{"cell_type":"markdown","metadata":{"id":"huU-Y9ye8Dv2"},"source":["### Get Run Name"]},{"cell_type":"code","metadata":{"id":"jS2UST3k8LeU"},"source":["def get_run_name_time(seed, model, criterion, optimizer, comments, config=config):\n","    try:\n","        if criterion.name:\n","            p_criterion = criterion.name\n","    except:\n","        p_criterion = 'CE'\n","\n","    lr = optimizer.param_groups[0]['lr']\n","    wd = optimizer.param_groups[0]['weight_decay']\n","    p_optimizer = f'{str(optimizer).split(\"(\")[0].strip()}'\n","    p_optimizer += f'_lr{lr}'\n","\n","    tb = config.getint('TRAIN', 'batch')\n","    epochs = config.getint('TRAIN', 'epochs')\n","    vs = config.getfloat('VAL', 'split')\n","    vb = config.getint('VAL', 'batch')\n","    sch_name = config.get('TRAIN', 'scheduler')\n","    if comments:\n","        comments = \"_\" + comments\n","\n","    p_scheduler = f'{sch_name}'\n","    \n","    run_name = f'{model.name}_{seed}_e{epochs}_' \\\n","                + f'tb{tb}_vs{vs}_vb{vb}_' \\\n","                + f'{p_criterion}_{p_optimizer}_' \\\n","                + f'{p_scheduler}' \\\n","                + f'{comments}' \n","                \n","    run_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    print(run_name, run_time)\n","    return run_name, run_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dKWwmvalkLs"},"source":["def setup_train_directories(run_name, run_time):\n","    checkpt_dir = pConfig['checkpoint_dir'] \n","    run_cpt_dir = os.path.join(checkpt_dir, run_name, run_time)\n","    if not os.path.exists(run_cpt_dir):\n","        os.makedirs(run_cpt_dir)\n","    pConfig['checkpoint_fname'] = os.path.join(run_cpt_dir, 'checkpoint.pth.tar')\n","    pConfig['bestmodel_fname'] = os.path.join(run_cpt_dir, 'model_best.pth.tar')\n","    pConfig['final_model_fname'] = os.path.join(run_cpt_dir, 'model_final.pth.tar') #trainfullmodel_fname\n","\n","    utils.update_config_to_file(os.path.join(run_cpt_dir, 'config.ini'), config)\n","\n","# setup_train_directories(run_name, run_time)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S_uFKvLuoVFi"},"source":["### Training Configurations"]},{"cell_type":"code","metadata":{"id":"MbiSCoHPoW9a"},"source":["# Training Configurations\n","dConfig['model'] = 'AJCNN8'\n","dConfig['evaluate'] = 'False'\n","valCfg['split'] = '0.2'\n","\n","dConfig['optimizer'] = 'SGD'\n","dConfig['lr'] = '0.01'\n","dConfig['criterion'] = 'SmoothCrossEntropyLoss'\n","\n","trainCfg['epochs'] = '60'\n","trainCfg['augments'] = 'None'\n","\n","# trainCfg['es_min'] = '10'\n","trainCfg['scheduler'] = 'StepLR'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CM6Dt7D39uXB"},"source":["### Training Loop"]},{"cell_type":"code","metadata":{"id":"SCMHgERleyZ1"},"source":["mnist_datasets, dataloaders = load_mnist_datasets(augments = augCfg, visualize=None)\n","model = create_model(dConfig['model'], device, verbose=True)\n","criterion = get_criterion(dConfig['criterion'], device)\n","# criterion = SmoothCrossEntropyLoss(smoothing=0.003)\n","\n","print(f\"=> Training model: {not dConfig.getboolean('evaluate')}\")\n","\n","if not dConfig.getboolean('evaluate'):\n","    optimizer = get_optimizer(model, config['DEFAULT']['optimizer'])\n","    run_name, run_time = get_run_name_time(seed, model, criterion, optimizer, comments)\n","    setup_train_directories(run_name, run_time)\n","    mtimer = trainer(dataloaders, model, criterion, optimizer, device, run_name)\n","    print(f\"=> Model trained time: {mtimer}\")\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pYPA_QwulJ4"},"source":["dConfig['evaluate'] = 'True'\n","if dConfig.getboolean('evaluate'):\n","    mnist_datasets, dataloaders = load_mnist_datasets(visualize='test')\n","    model, best_val_acc, total_time = load_ckp(pConfig['bestmodel_fname'], device)\n","    val_losses, val_accs = validate(dataloaders['test'], model, criterion, device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FosfGreh38PA"},"source":["## Train model with full training data with best cross-val hyper-params"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"GcMGomu3pHIu"},"source":["# Post tuning configurations\n","dConfig['evaluate'] = 'False'\n","valCfg['split'] = '0' # Use full train set for training"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"lGau-71ixhUW"},"source":["def trainer_final(dataloaders, model, criterion, optimizer, device, run_name, config=config):\n","\n","    # load best hyper-parameters from best model\n","    print(f\"=> loading best model hyper-parameters '{pConfig['bestmodel_fname']}'\")\n","    checkpoint = torch.load(pConfig['bestmodel_fname'])\n","\n","    # visualization\n","    model_timer = ModelTimer()\n","    scheduler = get_scheduler(optimizer, config['TRAIN']['scheduler'], config=config['TRAIN'])\n","\n","    epochs = checkpoint['epoch'] \n","    for epoch in range(epochs):\n","        model_timer.start_epoch_timer()\n","        \n","        lr = optimizer.param_groups[0]['lr']\n","\n","        print('\\nEpoch: [%d | %d] LR: %.16f' % (epoch + 1, epochs, lr))\n","\n","        # train for one epoch\n","        train_losses, train_accs = train(dataloaders['train'], model, criterion, \n","                                         optimizer, device)\n","        \n","        if config['TRAIN']['scheduler'] == 'ReduceLROnPlateau':\n","            # evaluate on test set\n","            val_losses, val_accs = validate(dataloaders['valid'], model, criterion, \n","                                            device)\n","            scheduler.step(val_losses.avg)\n","        else:\n","            scheduler.step()  \n","\n","        model_timer.stop_epoch_timer()\n","\n","    is_best = False \n","    save_ckp({\n","        'epoch': epoch + 1,\n","        'arch': model.name,\n","        'state_dict': model.state_dict(),\n","        'opt_name': config[\"DEFAULT\"][\"optimizer\"],\n","        'optimizer' : optimizer.state_dict(),\n","        'lr': lr,\n","        'total_time': model_timer.total_time,\n","        'scheduler': config[\"TRAIN\"][\"scheduler\"],\n","        'criterion': config[\"DEFAULT\"][\"criterion\"],\n","        'best_val_acc': None,\n","    }, is_best, pConfig['final_model_fname'], None)\n","        \n","\n","    print(\"Training completed!\")\n","    print(\"Train full model saved!\")\n","    return model_timer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ccx5p_Fkg3K"},"source":["mnist_datasets, dataloaders = load_mnist_datasets(augments = augCfg, visualize=None)\n","model = create_model(dConfig['model'], device, verbose=False)\n","# criterion = get_criterion(config['DEFAULT']['criterion'], device)\n","print(f'criterion: {criterion}')\n","\n","print(f\"=> Training model: {not dConfig.getboolean('evaluate')}\")\n","\n","if not dConfig.getboolean('evaluate'):\n","    optimizer = get_optimizer(model, config['DEFAULT']['optimizer'])\n","    mtimer = trainer_final(dataloaders, model, criterion, optimizer, device, run_name)\n","    print(f\"=> Model trained time: {mtimer}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y_MbjPf2OApB"},"source":["## Evaluate on Testset"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"A8RjmIUoeycW"},"source":["dConfig['evaluate'] = 'True'\n","# pConfig['final_model_fname'] = 'checkpoints/AJCNN8_42_e60_tb128_vs0.2_vb64_CE_SGD_lr0.01_StepLR/20201127_030058/model_final.pth.tar'\n","if dConfig.getboolean('evaluate'):\n","    mnist_datasets, dataloaders = load_mnist_datasets(visualize='test')\n","    # model, best_val_acc, total_time = load_ckp(pConfig['bestmodel_fname'], device)\n","    model, best_val_acc, total_time = load_ckp(pConfig['final_model_fname'], device)\n","    val_losses, val_accs = validate(dataloaders['test'], model, criterion, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WrgQfGclJWw-"},"source":["def test_final(test_loader, model):\n","    y_preds = []\n","    y_labels = []\n","\n","    # switch to evaluate mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","        end = time.time()\n","        for i, (X, y) in enumerate(tqdm(test_loader)):\n","            # Overlapping transfer if pinned memory\n","            X = X.to(device, non_blocking=True)\n","            y = y.to(device, non_blocking=True)\n","            \n","            # compute output\n","            output = model(X)\n","\n","            predicted_labels = output.argmax(dim=1)\n","            y_preds.extend(predicted_labels.detach().cpu().numpy())\n","            y_labels.extend(y.detach().cpu().numpy())\n","    \n","    preds_df = pd.DataFrame({\n","        'y_pred': y_preds,\n","        'y_label': y_labels,\n","    })\n","    return preds_df\n","\n","def print_numbers_acc(accs, numbers=range(10)):\n","    assert len(accs) == len(numbers)\n","    assert type(accs[0]) == AverageMeter\n","    for t, a in zip(accs, numbers):\n","        print(f\"{a}: {t.avg}\")\n","    return {a:t.avg.item() for t, a in zip(accs, numbers)}\n","\n","preds_df = test_final(dataloaders['test'], model)\n","preds_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"lvQSq7G-IOPu"},"source":["corr_preds = preds_df[preds_df['y_pred'] == preds_df['y_label']]\n","corr_count = dict(zip(range(10),list(corr_preds['y_label'].value_counts().sort_index())))\n","for k,v in corr_count.items():\n","    corr_count[k] = v/preds_df[preds_df['y_label'] == k]['y_label'].count() * 100\n","\n","corr_count "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"kBEhz8CHZD6S"},"source":["error_preds = preds_df[preds_df['y_pred'] != preds_df['y_label']]\n","error_preds.transpose()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"slF2FvVrUZiv"},"source":["fig, ax = plt.subplots()\n","pd.value_counts(error_preds['y_label']).sort_index().plot(\n","    title='Model Error Count (by digit)',\n","    kind='bar', figsize=(10, 5), ax=ax, color='darkred', xlabel=\"digit\", ylabel=\"count\")\n","\n","for p in ax.patches:\n","    value = round(p.get_height(),2)\n","    ax.annotate(str(value), xy=(p.get_x()+0.2, p.get_height()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"_-ikDRfqnQD8"},"source":["def plot_sample_images(X, y, label, images_to_show=10, random=True):\n","    fig = plt.figure(1)\n","    # Set the canvas based on the numer of images\n","    fig.set_size_inches(18.5, images_to_show * 0.3)\n","\n","    images_to_show = min(len(X), images_to_show)\n","\n","    # Generate random integers (non repeating)\n","    if random == True:\n","        idx = np.random.choice(len(X), images_to_show, replace=False)\n","    else:\n","        idx = np.arange(images_to_show)\n","        \n","    # Print the images with labels\n","    for i in range(images_to_show):\n","        plt.subplot(images_to_show/10 + 1, 10, i+1)\n","        plt.title(f\"Predict: {str(y[idx[i]])}\\n GT: {label}\")\n","        plt.imshow(X[idx[i]].reshape(28,28), cmap='gray')\n","        plt.axis('off') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Dvb0hLnniRC"},"source":["error_preds_by_labels = dict.fromkeys(error_preds['y_label'].sort_index())\n","for i in error_preds_by_labels.keys():\n","    error_preds_by_labels[i] = list(zip(error_preds.loc[error_preds['y_label'] == i].index, error_preds.loc[error_preds['y_label'] == i, 'y_pred']))\n","\n","# print(error_preds_by_labels)\n","for label, errors in error_preds_by_labels.items():\n","    # print(label, errors)\n","    error_img_indices = [x[0] for x in errors]\n","    mislabels = [x[1] for x in errors]\n","    error_images = [mnist_datasets['test'][i][0] for i in error_img_indices]\n","    error_images = torch.stack(error_images)\n","    # print(error_images)\n","    plot_sample_images(error_images, mislabels, label, 10, random=False)\n","    plt.savefig(f'error_analysis-{label}.png', bbox_inches='tight')\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_isuCTeSjTa3"},"source":["# Tensorboard"]},{"cell_type":"code","metadata":{"id":"V4lgO5Dceyeq"},"source":["%tensorboard --logdir runs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ny26Jgf7eyhI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTz9lPQneyjU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NpIJFQkeylt"},"source":[""],"execution_count":null,"outputs":[]}]}